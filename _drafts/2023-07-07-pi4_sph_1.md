---
layout: post
title: An amateur to Smoothed Particle Hydrodynamics (SPH) runs SPH on a Raspberry Pi 4 (Part 1)
tags: []
---

<!-- choose tags, and maybe a better title -->

In the summer of 2021, I wanted to make a device that would let me toss around an ocean of water in my hands. Back then, after throwing a couple of months at the idea, I only managed to implement the [simple fluid simulation]() that was devised in Jos Stam's 2003 ["Real-time Fluid Dynamics for Games"](). That can be thought of as a flat bed of water stirring or a cloud of smoke wafting about in the air, but it was no ocean---waves crashing into the side of the box.

Two years later, I threw one more month at the problem, and I managed to put together exactly what I was looking for.

<!-- VIDEO HERE -->

This video shows a Raspberry Pi 4 running at 100% on all four of its cores! Not to mention that the program it's running was written in under 750 lines of C! Even so, it reads the direction of gravity from the MPU6050 driver, simulates a blob of water falling in that direction, then sends a render of that blob to the SSD1306 driver. I always find it great when so much can be packed into so little!

What's been implemented here is a baseline variety of "smoothed particle hydrodynamics" (SPH), a class of approaches to simulating a fluid by representing it with discrete particles. One strength is of SPH methods over other fluid simulation methods is that they capture "free-surface flows" easily enough. Tossing around a figurative ocean is just that kind of problem. SPH is also a mature field. Really, the bulk of this specific baseline I implemented was first derived decades ago, and I'm sure that more modern incarnations can do the same faster and with richer output.

Still, to fully explain what happens here from scratch will take more than a single post already, let alone me trying to implement anything more advanced then trying to explain that on top. And I'm interested in being thorough here. Back in 2021, I could count on the wealth of materials on the clearnet that came about from Jos Stam's paper. These were people who implemented his method then wrote about their *implementation* comprehensively. But this time, I ended up picking up this or that detail from this or that research paper, and not many writeups about an end-to-end implementation of SPH were to be found.

So, though I am an amateur who's understanding of SPH only comes from 15 or so academic publications on it, here's how I packed SPH into a Raspberry Pi 4. This first out of two posts is about the "summation interpolant" and my choice out of the known approaches to estimating density, and the next part is about how I approached the SPH physics and rendering.

What's common to all SPH methods is the "summation interpolant", an approximation of a continuous function constructed from a sum over a set of discrete particles. Using this approximation, we can still get the value of some property at any single point in the fluid. Almost every SPH paper reviews how it's derived or points to a source that does. Here, we can turn to a [2005 review]() by J. J. Monaghan, who was one of the first to derive SPH in 1977. Incidentally, you can find the same materials in a pair of lectures he gave that was recorded and [posted to Youtube](). Formally, the value of some property at any point in the fluid can be written as a function of the point's coordinates, $A(\vec{x})$. Next, we can consider taking the "convolution" of it with the multidimensional Dirac delta, recognizing that the sifting property of the Dirac delta means that we'd just get $A(\vec{x})$ back.

$$ \int A(\vec{y}) \delta(\vec{x}-\vec{y}) d \vec{y} = A(\vec{x}) $$

Next, we apply the first approximation, getting what Monaghan calls the "integral interpolant". We replace the Dirac delta with one out of a parameterized family of functions $W(\vec{y}-\vec{x}, h)$.

$$ A(\vec{x}) \approx \int A(\vec{y}) W(\vec{x} - \vec{y}, h) d \vec{y} $$

This parameterized function is called the "kernel", and there are many choices for the kernel. We can talk specific choices later in this post, but in any case they at least share the fact that (1) their area is equal to one for any $h$ and (2) as $h$ approaches zero, the kernel function approaches the Dirac delta. The unit area keeps the integral interpolant from looking like a multiple of $A(\vec{x})$, not to mention that the Dirac delta is also unit-area. A theoretical example would be the Gaussian function but normalized for unit area---like how it is when used as the probability density function of the multivariate normal distribution---though it's not used in the context of fluid simulators.

The second approximation is to turn the integral interpolant into the "summation interpolant". Instead of approximating the integral by---say---the Riemann sum over square prisms of finite width, we approximate the integral with a sum over particles of finite mass. There's a specific way this is written in Monaghan's review, but I'll add my own steps in between in order to comment on it a bit.

$$ \int A(\vec{y}) W(\vec{x}-\vec{y}) d \vec{y} = \int \frac{A(\vec{y})}{\rho(\vec{y})} W(\vec{x}-\vec{y}, h) \rho(\vec{y}) d \vec{y} $$

First is a bit of an algebraic maneuver. Here, we introduce a new continuous function that represents the density of the fluid at any point, $\rho(\vec{y})$. This function isn't actually defined yet, and rather it's been supposed to represent some other property. We'll take advantage of this later to estimate the density itself. By multiplying the denominator (being one) and the numerator by this density function, we get the product $\rho(\vec{y}) d \vec{y}$, or the infinitesimal volume times the density---meaning the infinitesimal mass. Next, we can replace that with finite mass, getting 

$$ \int \frac{A(\vec{y})}{\rho(\vec{y})} W(\vec{x}-\vec{y}, h) \rho(\vec{y}) d \vec{y} \approx \sum_j m_j \frac{A(\vec{x}_j)}{\rho(\vec{x}_j)} W(\vec{x}-\vec{x}_j, h) $$

where $\vec{x}_j$ is the coordinates of particle $j$.

Because the summation interpolant now only uses the "some property" function $A(\vec{x})$ taken at the coordinates' of the particles $\vec{x}_j$, we can just have that $A(\vec{x}_j)$ be carried the particles themselves. Following this change in perspective, it gets rewritten as $A_j$. Technically, the same can be said for the density function $\rho(\vec{x})$, and in some rare cases this is actually done (see Monaghan's 1994 paper ["Simulating Free Surface Flows with SPH"]() or the simple 2D dam break given by P. Ramachandran in his [2021 paper]() accompanying the PySPH project for example). However, Monaghan here goes with another approach: suppose $A(\vec{x})$ *also* represented density. In other words, $A(\vec{x}) = \rho(\vec{x})$. As a result, we can approximate the density---which gets used in the summation interpolant---with another summation interpolant!

$$ \rho(\vec{x}) \approx \sum_j m_j W(\vec{x}-\vec{x}_j, h) $$

Finally, though the density function at the coordinates of particle $j$, $\rho(\vec{x}_j)$, is treated differently from the "some property" function at those coordinates, $A(\vec{x}_j)$, it is often written as $\rho_j$ as if it also was a carried value. These treatments finally lead us to the summation interpolant derived by Monaghan.

$$ A(\vec{x}) \approx A_s(\vec{x}) = \sum_j m_j \frac{A_j}{\rho_j} W(\vec{x}-\vec{x}_j, h) $$

If this approximation is evaluated at the location of some particle $i$, $\vec{x} _i$, the kernel function in this expression is also often abbreviated from $W(\vec{x} _i-\vec{x} _j, h)$ to just $W _{ij}$.

This completes the definition of the summation interpolant, but before we move on to how it gets applied in theory and implemented in my C program, I want to point out a small wrinkle in this definition that tripped me up: in the calculation of the density of particle $i$, $\rho_i$, does particle $i$ contribute to itself? Or in other words, can $i = j$? You might think so, and I can point to the SPlishSPlasH library [unequivocally saying yes](). However, across the papers I've read, I've seen the summation $\sum_j$ being interpreted as particle $i$ taking contributions from it's "neighbors" $j$. Common sense tells us that something does not neighbor itself. Coincidentally, a testing routine in the "neighborhood search" library that's used by SPlishSPlasH [unequivocally backs this notion](), and from there its calculation of the pressure gradient (a matter for the next post) at particle $i$ [does not take from particle $i$ itself in the summation](). So, is that a yes or no? Right now, I only know this amateur's point of view: the summation $\sum_j$ seems to have two meanings. In the general case $i \not= j$, but calculating the density estimate is an exception where a particle can "neighbor" itself. In any case, I've tried both scenarios, where $\rho_i$ takes from itself or doesn't, and the latter run blew up eventually.

If there are two meanings, we could abolish the second meaning if we wrote the density approximation as

$$ \rho(\vec{x}_i) \approx m_i W(\vec{0}, h) + \sum_j m_j W(\vec{x}_i - \vec{x}_j, h) $$

which is consistent with the first meaning where $i \not= j$. We could even be explicit about it if we wrote the summation as $\sum_{i \not= j}$.

This isn't the last word on density. Besides this wrinkle in the baseline density approximation that tripped me up, I've found many different cures people have tried to apply to the issues with it, from just clamping it to tweaking the summation interpolant to ditching the summation interpolant altogether, not to mention the "rare" choice of using a carried density. All this is about a core piece of SPH! However, I'll leave the way I chose to deal with the density approximation in the next part. Spoiler: I pretty much took the clamp.

That aside, a most convenient fact about the summation interpolant is that derivatives of it can be expressed exactly! Because the only part that depends on $\vec{x}$ is the kernel, we can just roll the differentiation over. Letting $x$ be some component of the vector $\vec{x}$, we can state the partial derivative to be

$$ \frac{\partial A_s}{\partial x} = \sum_j m_j \frac{A_j}{\rho_j} \frac{\partial}{\partial x} \Big( W(\vec{x} - \vec{x}_j, h) \Big) $$

where $\frac{\partial}{\partial x}( W(\vec{x}-\vec{x}_j, h) )$ is just a known function of $\vec{x}$ that depends on what choice of the kernel $W$ we take.

I won't oversell this, however. Some treatment of the continuous function before applying the summation interpolant and then taking the derivative can be useful. For example, Monaghan creates a specific derivation of the gradient of the pressure (again, next part) that still conserves momentum after converting it with the summation interpolant. In fact, a whole collection of derivations of the gradient, divergence, and so on along with where they're useful can be found in a [2022 survey]() by D. Koshier, though they aren't used here. That survey also shows that Monaghan's derivation has survived to this day.

The last matters I'll cover for today is the choice of a kernel with a "compact support" and implementing a "neighborhood search".

There is a third thing that most kernels share: a "compact support", or in other words the fact that they evaluate to zero beyond some threshold distance. Choosing a kernel that has this property means that some particles contribute a term *that is equal to zero* to summation approximation. In other words, the particle *doesn't contribute*, and if we see that the distance is past that point, we can safely skip calculating that contribution. This sets up the basis for an incredible fast path that involves a "neighborhood search".

Take the Gaussian kernel again, which does not have this property. It never hits zero for any distance, and as a result using it would mean *all* particles contribute to the summation interpolant at a single point, no matter how far the particle is and how small the contribution will be. On the other hand, one family of functions that does have this property is the "$M_4$ splines", also called the "cubic splines". Back in 2005, Monaghan recognized in his review that this choice had survived the decades since it was introduced, and then by 2015 even, Jan Bender published ["Divergence-Free Smoothed Particle Hydrodynamics"]() (that's the paper's title and the name of his method) where he continued to use the cubic spline. On the other hand, after reading about the "Wendland" functions from a 2012 book by Damien Violeau (that's [*Fluid Mechanics and the SPH Method: Theory and Applications*]()), I happened to end up using that. They also have this property. I leave here the definition of the Wendland C2 family, normalized for two dimensions:

$$ W(\vec{x}_i - \vec{x}_j, h) = \begin{cases} \frac{7}{4 \pi h^2} (1-0.5 q)^4 (1+2 q) & q < 2 \\ 0 & q \geq 2 \end{cases} $$

where $q = \left\Vert \vec{x}_i - \vec{x}_j \right\Vert / h$. I'll mention in case it's needed that, for three dimensions, replace $7/(4 \pi h^2)$ with $21/(16 \pi h^3)$. I'll also leave here its derivative with respect to $q$:

$$ \frac{dW}{dq} = \begin{cases} \frac{7}{4 \pi h^2} (-5q) (1-0.5 q)^3 & q < 2 \\ 0 & q \geq 2 \end{cases} $$

Then, the derivative with respect to some component $x_i$ of the vector $\vec{x}_i$ can be recovered by the chain rule

$$ \frac{\partial W}{\partial x_i} = \frac{dW}{dq} \frac{dq}{d x_i} $$

and, for two dimensions, $\partial q / \partial x$ is found to be

$$ \frac{\partial q}{\partial x_i} = \frac{x_i - x_j}{\left\Vert \vec{x}_i - \vec{x}_j \right\Vert h} $$

where this definition happens to hold across kernels!

Notice here that its distance threshold is $2 h$ and beyond which the kernel evaluates to zero. This figure is important to the "neighborhood search" and a special data structure that it depends on, called the "cell linked-list". Even in 1994 did Monaghan use this in his free-surfaces paper, but I find a 2010 research paper by J. M. Dominguez to cover it more comprehensively. The point of it is this: given some distance threshold (here being $2h$), if we split the domain into a grid of square cells with a width equal to that threshold, then all possible "neighbors" are in the cell containing the point we query with and also the neighboring cells. And again, if that point comes from a particle, that particle itself usually doesn't count.

<!-- diagram here -->

So, how do we exploit this point? If we associate each cell with a kind of list that contains references to each and every particle that cell contains, then we'd only need to test the handful of particles referred to by a handful of cells. We save ourselves from testing every distance in the first place! On this matter P. Ramachadran gives an explicit set of instructions in the PySPH paper. Two arrays: the `next` array contains indices for where to next in `next` itself, and the `head` array contains indices for where to start in `next`. At the same time, these indices also point to a specific particle.

<!-- diagram here -->

It's not even a canonical linked list, really, but it's quite the minimal thing that we can get away with in this context. Of course, we can also use real linked lists. In fact, that was what I did before, but this was either slightly faster or dramatically faster.

<!-- Add the matter of optionally going for vectorization (SPlisHSPlasH does this with custom AVX codes, but I went with associative floating point and autovectorization to keep it simple) -->

<!-- Matter of data/loop parallelism over neighbors? -->

That about covers my implementation of the summation approximant and how I used it to estimate the density, including the matter of kernel choice and neighborhood search. Because I'm no expert in SPH, I stuck to the research where I could as I was explaining this project, and even then I'd advocate for confirming your impressions by reading the research itself if you can. Still, I had to venture out a bit because of the wrinkle in the density definition, and how the concepts became C code was mostly my own doing. I documented this as well, and I expect to do that again. In the end, the research and my implementation are the basis for this post and the next post.

Once I'm done with it you'll be able to find the link here, but there's always a link to my code and equations here if you're reading this before then. The research has been linked to already.